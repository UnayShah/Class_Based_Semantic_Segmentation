{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader for Cityscape Dataset ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2975 images in the folder ./datasets/citys\\leftImg8bit/train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\unays\\AppData\\Local\\Temp\\ipykernel_12180\\1094943380.py:118: DeprecationWarning: FLIP_LEFT_RIGHT is deprecated and will be removed in Pillow 10 (2023-07-01). Use Transpose.FLIP_LEFT_RIGHT instead.\n",
      "  img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
      "C:\\Users\\unays\\AppData\\Local\\Temp\\ipykernel_12180\\1094943380.py:119: DeprecationWarning: FLIP_LEFT_RIGHT is deprecated and will be removed in Pillow 10 (2023-07-01). Use Transpose.FLIP_LEFT_RIGHT instead.\n",
      "  mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
      "C:\\Users\\unays\\AppData\\Local\\Temp\\ipykernel_12180\\1094943380.py:130: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  img = img.resize((ow, oh), Image.BILINEAR)\n",
      "C:\\Users\\unays\\AppData\\Local\\Temp\\ipykernel_12180\\1094943380.py:131: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  mask = mask.resize((ow, oh), Image.NEAREST)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Cityscapes Dataloader\"\"\"\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "\n",
    "__all__ = ['CitySegmentation']\n",
    "\n",
    "\n",
    "class CitySegmentation(data.Dataset):\n",
    "    \"\"\"Cityscapes Semantic Segmentation Dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    root : string\n",
    "        Path to Cityscapes folder. Default is './datasets/citys'\n",
    "    split: string\n",
    "        'train', 'val' or 'test'\n",
    "    transform : callable, optional\n",
    "        A function that transforms the image\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from torchvision import transforms\n",
    "    >>> import torch.utils.data as data\n",
    "    >>> # Transforms for Normalization\n",
    "    >>> input_transform = transforms.Compose([\n",
    "    >>>     transforms.ToTensor(),\n",
    "    >>>     transforms.Normalize((.485, .456, .406), (.229, .224, .225)),\n",
    "    >>> ])\n",
    "    >>> # Create Dataset\n",
    "    >>> trainset = CitySegmentation(split='train', transform=input_transform)\n",
    "    >>> # Create Training Loader\n",
    "    >>> train_data = data.DataLoader(\n",
    "    >>>     trainset, 4, shuffle=True,\n",
    "    >>>     num_workers=4)\n",
    "    \"\"\"\n",
    "    BASE_DIR = 'cityscapes'\n",
    "    NUM_CLASS = 19\n",
    "\n",
    "    def __init__(self, root='./datasets/citys', split='train', mode=None, transform=None,\n",
    "                 base_size=520, crop_size=480, **kwargs):\n",
    "        super(CitySegmentation, self).__init__()\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        self.mode = mode if mode is not None else split\n",
    "        self.transform = transform\n",
    "        self.base_size = base_size\n",
    "        self.crop_size = crop_size\n",
    "        self.images, self.mask_paths = _get_city_pairs(self.root, self.split)\n",
    "        assert (len(self.images) == len(self.mask_paths))\n",
    "        if len(self.images) == 0:\n",
    "            raise RuntimeError(\"Found 0 images in subfolders of: \" + self.root + \"\\n\")\n",
    "        self.valid_classes = [7, 8, 11, 12, 13, 17, 19, 20, 21, 22,\n",
    "                              23, 24, 25, 26, 27, 28, 31, 32, 33]\n",
    "        self._key = np.array([-1, -1, -1, -1, -1, -1,\n",
    "                              -1, -1, 0, 1, -1, -1,\n",
    "                              2, 3, 4, -1, -1, -1,\n",
    "                              5, -1, 6, 7, 8, 9,\n",
    "                              10, 11, 12, 13, 14, 15,\n",
    "                              -1, -1, 16, 17, 18])\n",
    "        self._mapping = np.array(range(-1, len(self._key) - 1)).astype('int32')\n",
    "\n",
    "    def _class_to_index(self, mask):\n",
    "        values = np.unique(mask)\n",
    "        for value in values:\n",
    "            assert (value in self._mapping)\n",
    "        index = np.digitize(mask.ravel(), self._mapping, right=True)\n",
    "        return self._key[index].reshape(mask.shape)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        if self.mode == 'test':\n",
    "            if self.transform is not None:\n",
    "                img = self.transform(img)\n",
    "            return img, os.path.basename(self.images[index])\n",
    "        mask = Image.open(self.mask_paths[index])\n",
    "        # synchrosized transform\n",
    "        if self.mode == 'train':\n",
    "            img, mask = self._sync_transform(img, mask)\n",
    "        elif self.mode == 'val':\n",
    "            img, mask = self._val_sync_transform(img, mask)\n",
    "        else:\n",
    "            assert self.mode == 'testval'\n",
    "            img, mask = self._img_transform(img), self._mask_transform(mask)\n",
    "        # general resize, normalize and toTensor\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, mask\n",
    "\n",
    "    def _val_sync_transform(self, img, mask):\n",
    "        outsize = self.crop_size\n",
    "        short_size = outsize\n",
    "        w, h = img.size\n",
    "        if w > h:\n",
    "            oh = short_size\n",
    "            ow = int(1.0 * w * oh / h)\n",
    "        else:\n",
    "            ow = short_size\n",
    "            oh = int(1.0 * h * ow / w)\n",
    "        img = img.resize((ow, oh), Image.BILINEAR)\n",
    "        mask = mask.resize((ow, oh), Image.NEAREST)\n",
    "        # center crop\n",
    "        w, h = img.size\n",
    "        x1 = int(round((w - outsize) / 2.))\n",
    "        y1 = int(round((h - outsize) / 2.))\n",
    "        img = img.crop((x1, y1, x1 + outsize, y1 + outsize))\n",
    "        mask = mask.crop((x1, y1, x1 + outsize, y1 + outsize))\n",
    "        # final transform\n",
    "        img, mask = self._img_transform(img), self._mask_transform(mask)\n",
    "        return img, mask\n",
    "\n",
    "    def _sync_transform(self, img, mask):\n",
    "        # random mirror\n",
    "        if random.random() < 0.5:\n",
    "            img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        crop_size = self.crop_size\n",
    "        # random scale (short edge)\n",
    "        short_size = random.randint(int(self.base_size * 0.5), int(self.base_size * 2.0))\n",
    "        w, h = img.size\n",
    "        if h > w:\n",
    "            ow = short_size\n",
    "            oh = int(1.0 * h * ow / w)\n",
    "        else:\n",
    "            oh = short_size\n",
    "            ow = int(1.0 * w * oh / h)\n",
    "        img = img.resize((ow, oh), Image.BILINEAR)\n",
    "        mask = mask.resize((ow, oh), Image.NEAREST)\n",
    "        # pad crop\n",
    "        if short_size < crop_size:\n",
    "            padh = crop_size - oh if oh < crop_size else 0\n",
    "            padw = crop_size - ow if ow < crop_size else 0\n",
    "            img = ImageOps.expand(img, border=(0, 0, padw, padh), fill=0)\n",
    "            mask = ImageOps.expand(mask, border=(0, 0, padw, padh), fill=0)\n",
    "        # random crop crop_size\n",
    "        w, h = img.size\n",
    "        x1 = random.randint(0, w - crop_size)\n",
    "        y1 = random.randint(0, h - crop_size)\n",
    "        img = img.crop((x1, y1, x1 + crop_size, y1 + crop_size))\n",
    "        mask = mask.crop((x1, y1, x1 + crop_size, y1 + crop_size))\n",
    "        # gaussian blur as in PSP\n",
    "        if random.random() < 0.5:\n",
    "            img = img.filter(ImageFilter.GaussianBlur(\n",
    "                radius=random.random()))\n",
    "        # final transform\n",
    "        img, mask = self._img_transform(img), self._mask_transform(mask)\n",
    "        return img, mask\n",
    "\n",
    "    def _img_transform(self, img):\n",
    "        return np.array(img)\n",
    "\n",
    "    def _mask_transform(self, mask):\n",
    "        target = self._class_to_index(np.array(mask).astype('int32'))\n",
    "        return torch.LongTensor(np.array(target).astype('int32'))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    @property\n",
    "    def num_class(self):\n",
    "        \"\"\"Number of categories.\"\"\"\n",
    "        return self.NUM_CLASS\n",
    "\n",
    "    @property\n",
    "    def pred_offset(self):\n",
    "        return 0\n",
    "\n",
    "\n",
    "def _get_city_pairs(folder, split='train'):\n",
    "    def get_path_pairs(img_folder, mask_folder):\n",
    "        img_paths = []\n",
    "        mask_paths = []\n",
    "        for root, _, files in os.walk(img_folder):\n",
    "            for filename in files:\n",
    "                if filename.endswith(\".png\"):\n",
    "                    imgpath = os.path.join(root, filename)\n",
    "                    foldername = os.path.basename(os.path.dirname(imgpath))\n",
    "                    maskname = filename.replace('leftImg8bit', 'gtFine_labelIds')\n",
    "                    maskpath = os.path.join(mask_folder, foldername, maskname)\n",
    "                    if os.path.isfile(imgpath) and os.path.isfile(maskpath):\n",
    "                        img_paths.append(imgpath)\n",
    "                        mask_paths.append(maskpath)\n",
    "                    else:\n",
    "                        print('cannot find the mask or image:', imgpath, maskpath)\n",
    "        print('Found {} images in the folder {}'.format(len(img_paths), img_folder))\n",
    "        return img_paths, mask_paths\n",
    "\n",
    "    if split in ('train', 'val'):\n",
    "        img_folder = os.path.join(folder, 'leftImg8bit/' + split)\n",
    "        mask_folder = os.path.join(folder, 'gtFine/' + split)\n",
    "        img_paths, mask_paths = get_path_pairs(img_folder, mask_folder)\n",
    "        return img_paths, mask_paths\n",
    "    else:\n",
    "        assert split == 'trainval'\n",
    "        print('trainval set')\n",
    "        train_img_folder = os.path.join(folder, 'leftImg8bit/train')\n",
    "        train_mask_folder = os.path.join(folder, 'gtFine/train')\n",
    "        val_img_folder = os.path.join(folder, 'leftImg8bit/val')\n",
    "        val_mask_folder = os.path.join(folder, 'gtFine/val')\n",
    "        train_img_paths, train_mask_paths = get_path_pairs(train_img_folder, train_mask_folder)\n",
    "        val_img_paths, val_mask_paths = get_path_pairs(val_img_folder, val_mask_folder)\n",
    "        img_paths = train_img_paths + val_img_paths\n",
    "        mask_paths = train_mask_paths + val_mask_paths\n",
    "    return img_paths, mask_paths\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset = CitySegmentation()\n",
    "    img, label = dataset[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'citys': CitySegmentation,\n",
    "}\n",
    "\n",
    "\n",
    "def get_segmentation_dataset(name, **kwargs):\n",
    "    \"\"\"Segmentation Datasets\"\"\"\n",
    "    return datasets[name.lower()](**kwargs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAST SCNN MODEL ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# Created by: Tramac\n",
    "# Date: 2019-03-25\n",
    "# Copyright (c) 2017\n",
    "###########################################################################\n",
    "\n",
    "\"\"\"Fast Segmentation Convolutional Neural Network\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "__all__ = ['FastSCNN', 'get_fast_scnn']\n",
    "\n",
    "\n",
    "class FastSCNN(nn.Module):\n",
    "    def __init__(self, num_classes, aux=False, **kwargs):\n",
    "        super(FastSCNN, self).__init__()\n",
    "        self.aux = aux\n",
    "        self.learning_to_downsample = LearningToDownsample(32, 48, 64)\n",
    "        self.global_feature_extractor = GlobalFeatureExtractor(64, [64, 96, 128], 128, 6, [3, 3, 3])\n",
    "        self.feature_fusion = FeatureFusionModule(64, 128, 128)\n",
    "        self.classifier = Classifer(128, num_classes)\n",
    "        if self.aux:\n",
    "            self.auxlayer = nn.Sequential(\n",
    "                nn.Conv2d(64, 32, 3, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(True),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Conv2d(32, num_classes, 1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.size()[2:]\n",
    "        higher_res_features = self.learning_to_downsample(x)\n",
    "        x = self.global_feature_extractor(higher_res_features)\n",
    "        x = self.feature_fusion(higher_res_features, x)\n",
    "        x = self.classifier(x)\n",
    "        outputs = []\n",
    "        x = F.interpolate(x, size, mode='bilinear', align_corners=True)\n",
    "        outputs.append(x)\n",
    "        if self.aux:\n",
    "            auxout = self.auxlayer(higher_res_features)\n",
    "            auxout = F.interpolate(auxout, size, mode='bilinear', align_corners=True)\n",
    "            outputs.append(auxout)\n",
    "        return tuple(outputs)\n",
    "\n",
    "\n",
    "class _ConvBNReLU(nn.Module):\n",
    "    \"\"\"Conv-BN-ReLU\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, **kwargs):\n",
    "        super(_ConvBNReLU, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class _DSConv(nn.Module):\n",
    "    \"\"\"Depthwise Separable Convolutions\"\"\"\n",
    "\n",
    "    def __init__(self, dw_channels, out_channels, stride=1, **kwargs):\n",
    "        super(_DSConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(dw_channels, dw_channels, 3, stride, 1, groups=dw_channels, bias=False),\n",
    "            nn.BatchNorm2d(dw_channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(dw_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class _DWConv(nn.Module):\n",
    "    def __init__(self, dw_channels, out_channels, stride=1, **kwargs):\n",
    "        super(_DWConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(dw_channels, out_channels, 3, stride, 1, groups=dw_channels, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class LinearBottleneck(nn.Module):\n",
    "    \"\"\"LinearBottleneck used in MobileNetV2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, t=6, stride=2, **kwargs):\n",
    "        super(LinearBottleneck, self).__init__()\n",
    "        self.use_shortcut = stride == 1 and in_channels == out_channels\n",
    "        self.block = nn.Sequential(\n",
    "            # pw\n",
    "            _ConvBNReLU(in_channels, in_channels * t, 1),\n",
    "            # dw\n",
    "            _DWConv(in_channels * t, in_channels * t, stride),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(in_channels * t, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block(x)\n",
    "        if self.use_shortcut:\n",
    "            out = x + out\n",
    "        return out\n",
    "\n",
    "\n",
    "class PyramidPooling(nn.Module):\n",
    "    \"\"\"Pyramid pooling module\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(PyramidPooling, self).__init__()\n",
    "        inter_channels = int(in_channels / 4)\n",
    "        self.conv1 = _ConvBNReLU(in_channels, inter_channels, 1, **kwargs)\n",
    "        self.conv2 = _ConvBNReLU(in_channels, inter_channels, 1, **kwargs)\n",
    "        self.conv3 = _ConvBNReLU(in_channels, inter_channels, 1, **kwargs)\n",
    "        self.conv4 = _ConvBNReLU(in_channels, inter_channels, 1, **kwargs)\n",
    "        self.out = _ConvBNReLU(in_channels * 2, out_channels, 1)\n",
    "\n",
    "    def pool(self, x, size):\n",
    "        avgpool = nn.AdaptiveAvgPool2d(size)\n",
    "        return avgpool(x)\n",
    "\n",
    "    def upsample(self, x, size):\n",
    "        return F.interpolate(x, size, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.size()[2:]\n",
    "        feat1 = self.upsample(self.conv1(self.pool(x, 1)), size)\n",
    "        feat2 = self.upsample(self.conv2(self.pool(x, 2)), size)\n",
    "        feat3 = self.upsample(self.conv3(self.pool(x, 3)), size)\n",
    "        feat4 = self.upsample(self.conv4(self.pool(x, 6)), size)\n",
    "        x = torch.cat([x, feat1, feat2, feat3, feat4], dim=1)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LearningToDownsample(nn.Module):\n",
    "    \"\"\"Learning to downsample module\"\"\"\n",
    "\n",
    "    def __init__(self, dw_channels1=32, dw_channels2=48, out_channels=64, **kwargs):\n",
    "        super(LearningToDownsample, self).__init__()\n",
    "        self.conv = _ConvBNReLU(3, dw_channels1, 3, 2)\n",
    "        self.dsconv1 = _DSConv(dw_channels1, dw_channels2, 2)\n",
    "        self.dsconv2 = _DSConv(dw_channels2, out_channels, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.dsconv1(x)\n",
    "        x = self.dsconv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GlobalFeatureExtractor(nn.Module):\n",
    "    \"\"\"Global feature extractor module\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels=64, block_channels=(64, 96, 128),\n",
    "                 out_channels=128, t=6, num_blocks=(3, 3, 3), **kwargs):\n",
    "        super(GlobalFeatureExtractor, self).__init__()\n",
    "        self.bottleneck1 = self._make_layer(LinearBottleneck, in_channels, block_channels[0], num_blocks[0], t, 2)\n",
    "        self.bottleneck2 = self._make_layer(LinearBottleneck, block_channels[0], block_channels[1], num_blocks[1], t, 2)\n",
    "        self.bottleneck3 = self._make_layer(LinearBottleneck, block_channels[1], block_channels[2], num_blocks[2], t, 1)\n",
    "        self.ppm = PyramidPooling(block_channels[2], out_channels)\n",
    "\n",
    "    def _make_layer(self, block, inplanes, planes, blocks, t=6, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(inplanes, planes, t, stride))\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(planes, planes, t, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bottleneck1(x)\n",
    "        x = self.bottleneck2(x)\n",
    "        x = self.bottleneck3(x)\n",
    "        x = self.ppm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeatureFusionModule(nn.Module):\n",
    "    \"\"\"Feature fusion module\"\"\"\n",
    "\n",
    "    def __init__(self, highter_in_channels, lower_in_channels, out_channels, scale_factor=4, **kwargs):\n",
    "        super(FeatureFusionModule, self).__init__()\n",
    "        self.scale_factor = scale_factor\n",
    "        self.dwconv = _DWConv(lower_in_channels, out_channels, 1)\n",
    "        self.conv_lower_res = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        self.conv_higher_res = nn.Sequential(\n",
    "            nn.Conv2d(highter_in_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        self.relu = nn.ReLU(True)\n",
    "\n",
    "    def forward(self, higher_res_feature, lower_res_feature):\n",
    "        lower_res_feature = F.interpolate(lower_res_feature, scale_factor=4, mode='bilinear', align_corners=True)\n",
    "        lower_res_feature = self.dwconv(lower_res_feature)\n",
    "        lower_res_feature = self.conv_lower_res(lower_res_feature)\n",
    "\n",
    "        higher_res_feature = self.conv_higher_res(higher_res_feature)\n",
    "        out = higher_res_feature + lower_res_feature\n",
    "        return self.relu(out)\n",
    "\n",
    "\n",
    "class Classifer(nn.Module):\n",
    "    \"\"\"Classifer\"\"\"\n",
    "\n",
    "    def __init__(self, dw_channels, num_classes, stride=1, **kwargs):\n",
    "        super(Classifer, self).__init__()\n",
    "        self.dsconv1 = _DSConv(dw_channels, dw_channels, stride)\n",
    "        self.dsconv2 = _DSConv(dw_channels, dw_channels, stride)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Conv2d(dw_channels, num_classes, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dsconv1(x)\n",
    "        x = self.dsconv2(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_fast_scnn(dataset='citys', pretrained=False, root='./weights', map_cpu=False, **kwargs):\n",
    "    acronyms = {\n",
    "        'pascal_voc': 'voc',\n",
    "        'pascal_aug': 'voc',\n",
    "        'ade20k': 'ade',\n",
    "        'coco': 'coco',\n",
    "        'citys': 'citys',\n",
    "    }\n",
    "\n",
    "\n",
    "    model = FastSCNN(datasets[dataset].NUM_CLASS, **kwargs)\n",
    "    if pretrained:\n",
    "        if(map_cpu):\n",
    "            model.load_state_dict(torch.load(os.path.join(root, 'fast_scnn_%s.pth' % acronyms[dataset]), map_location='cpu'))\n",
    "        else:\n",
    "            model.load_state_dict(torch.load(os.path.join(root, 'fast_scnn_%s.pth' % acronyms[dataset])))\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    img = torch.randn(2, 3, 256, 512)\n",
    "    model = get_fast_scnn('citys')\n",
    "    outputs = model(img)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SCRIPT FOR TRAINING  THE MODEL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2975 images in the folder ./datasets/citys\\leftImg8bit/train\n",
      "Found 500 images in the folder ./datasets/citys\\leftImg8bit/val\n",
      "w/ class balance\n",
      "Starting Epoch: 0, Total Epochs: 160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/160 [00:00<?, ?it/s]C:\\Users\\unays\\AppData\\Local\\Temp\\ipykernel_12180\\1094943380.py:118: DeprecationWarning: FLIP_LEFT_RIGHT is deprecated and will be removed in Pillow 10 (2023-07-01). Use Transpose.FLIP_LEFT_RIGHT instead.\n",
      "  img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
      "C:\\Users\\unays\\AppData\\Local\\Temp\\ipykernel_12180\\1094943380.py:119: DeprecationWarning: FLIP_LEFT_RIGHT is deprecated and will be removed in Pillow 10 (2023-07-01). Use Transpose.FLIP_LEFT_RIGHT instead.\n",
      "  mask = mask.transpose(Image.FLIP_LEFT_RIGHT)\n",
      "C:\\Users\\unays\\AppData\\Local\\Temp\\ipykernel_12180\\1094943380.py:130: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  img = img.resize((ow, oh), Image.BILINEAR)\n",
      "C:\\Users\\unays\\AppData\\Local\\Temp\\ipykernel_12180\\1094943380.py:131: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  mask = mask.resize((ow, oh), Image.NEAREST)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [ 0/160] Iter [  10/1487] || Time: 13.6506 sec || lr: 0.00999966 || Loss: 2.1001\n",
      "Epoch: [ 0/160] Iter [  20/1487] || Time: 22.8983 sec || lr: 0.00999928 || Loss: 2.0425\n",
      "Epoch: [ 0/160] Iter [  30/1487] || Time: 32.6826 sec || lr: 0.00999890 || Loss: 1.5249\n",
      "Epoch: [ 0/160] Iter [  40/1487] || Time: 42.8058 sec || lr: 0.00999852 || Loss: 1.9400\n",
      "Epoch: [ 0/160] Iter [  50/1487] || Time: 52.5275 sec || lr: 0.00999815 || Loss: 2.1841\n",
      "Epoch: [ 0/160] Iter [  60/1487] || Time: 62.2928 sec || lr: 0.00999777 || Loss: 1.5888\n",
      "Epoch: [ 0/160] Iter [  70/1487] || Time: 71.9017 sec || lr: 0.00999739 || Loss: 2.0418\n",
      "Epoch: [ 0/160] Iter [  80/1487] || Time: 81.5018 sec || lr: 0.00999701 || Loss: 1.8649\n",
      "Epoch: [ 0/160] Iter [  90/1487] || Time: 91.7918 sec || lr: 0.00999663 || Loss: 2.2252\n",
      "Epoch: [ 0/160] Iter [ 100/1487] || Time: 101.6713 sec || lr: 0.00999625 || Loss: 1.2424\n",
      "Epoch: [ 0/160] Iter [ 110/1487] || Time: 111.0659 sec || lr: 0.00999588 || Loss: 1.7068\n",
      "Epoch: [ 0/160] Iter [ 120/1487] || Time: 120.1856 sec || lr: 0.00999550 || Loss: 1.9300\n",
      "Epoch: [ 0/160] Iter [ 130/1487] || Time: 129.8171 sec || lr: 0.00999512 || Loss: 1.4965\n",
      "Epoch: [ 0/160] Iter [ 140/1487] || Time: 139.4427 sec || lr: 0.00999474 || Loss: 2.6190\n",
      "Epoch: [ 0/160] Iter [ 150/1487] || Time: 149.1135 sec || lr: 0.00999436 || Loss: 1.4927\n",
      "Epoch: [ 0/160] Iter [ 160/1487] || Time: 158.4306 sec || lr: 0.00999399 || Loss: 1.4059\n",
      "Epoch: [ 0/160] Iter [ 170/1487] || Time: 167.9804 sec || lr: 0.00999361 || Loss: 1.9069\n",
      "Epoch: [ 0/160] Iter [ 180/1487] || Time: 176.9363 sec || lr: 0.00999323 || Loss: 1.9541\n",
      "Epoch: [ 0/160] Iter [ 190/1487] || Time: 186.1450 sec || lr: 0.00999285 || Loss: 1.0605\n",
      "Epoch: [ 0/160] Iter [ 200/1487] || Time: 195.2586 sec || lr: 0.00999247 || Loss: 1.6902\n",
      "Epoch: [ 0/160] Iter [ 210/1487] || Time: 204.8244 sec || lr: 0.00999209 || Loss: 1.6745\n",
      "Epoch: [ 0/160] Iter [ 220/1487] || Time: 213.7896 sec || lr: 0.00999172 || Loss: 2.0648\n",
      "Epoch: [ 0/160] Iter [ 230/1487] || Time: 223.3293 sec || lr: 0.00999134 || Loss: 1.2637\n",
      "Epoch: [ 0/160] Iter [ 240/1487] || Time: 232.6513 sec || lr: 0.00999096 || Loss: 2.1607\n",
      "Epoch: [ 0/160] Iter [ 250/1487] || Time: 241.4811 sec || lr: 0.00999058 || Loss: 2.2785\n",
      "Epoch: [ 0/160] Iter [ 260/1487] || Time: 250.8692 sec || lr: 0.00999020 || Loss: 1.1639\n",
      "Epoch: [ 0/160] Iter [ 270/1487] || Time: 260.1787 sec || lr: 0.00998982 || Loss: 2.2288\n",
      "Epoch: [ 0/160] Iter [ 280/1487] || Time: 269.5695 sec || lr: 0.00998945 || Loss: 1.4881\n",
      "Epoch: [ 0/160] Iter [ 290/1487] || Time: 278.9789 sec || lr: 0.00998907 || Loss: 1.1910\n",
      "Epoch: [ 0/160] Iter [ 300/1487] || Time: 288.2856 sec || lr: 0.00998869 || Loss: 2.0016\n",
      "Epoch: [ 0/160] Iter [ 310/1487] || Time: 297.6726 sec || lr: 0.00998831 || Loss: 1.5384\n",
      "Epoch: [ 0/160] Iter [ 320/1487] || Time: 306.8803 sec || lr: 0.00998793 || Loss: 2.0994\n",
      "Epoch: [ 0/160] Iter [ 330/1487] || Time: 316.4469 sec || lr: 0.00998755 || Loss: 1.9211\n",
      "Epoch: [ 0/160] Iter [ 340/1487] || Time: 325.7687 sec || lr: 0.00998718 || Loss: 1.3254\n",
      "Epoch: [ 0/160] Iter [ 350/1487] || Time: 334.9654 sec || lr: 0.00998680 || Loss: 2.1357\n",
      "Epoch: [ 0/160] Iter [ 360/1487] || Time: 344.6286 sec || lr: 0.00998642 || Loss: 1.2587\n",
      "Epoch: [ 0/160] Iter [ 370/1487] || Time: 354.0417 sec || lr: 0.00998604 || Loss: 1.6197\n",
      "Epoch: [ 0/160] Iter [ 380/1487] || Time: 363.1693 sec || lr: 0.00998566 || Loss: 1.2579\n",
      "Epoch: [ 0/160] Iter [ 390/1487] || Time: 372.6491 sec || lr: 0.00998528 || Loss: 1.5207\n",
      "Epoch: [ 0/160] Iter [ 400/1487] || Time: 382.3607 sec || lr: 0.00998491 || Loss: 1.3762\n",
      "Epoch: [ 0/160] Iter [ 410/1487] || Time: 391.2054 sec || lr: 0.00998453 || Loss: 1.7210\n",
      "Epoch: [ 0/160] Iter [ 420/1487] || Time: 400.5275 sec || lr: 0.00998415 || Loss: 1.7150\n",
      "Epoch: [ 0/160] Iter [ 430/1487] || Time: 410.1752 sec || lr: 0.00998377 || Loss: 2.3422\n",
      "Epoch: [ 0/160] Iter [ 440/1487] || Time: 418.9792 sec || lr: 0.00998339 || Loss: 1.3174\n",
      "Epoch: [ 0/160] Iter [ 450/1487] || Time: 428.2747 sec || lr: 0.00998301 || Loss: 1.7522\n",
      "Epoch: [ 0/160] Iter [ 460/1487] || Time: 437.6505 sec || lr: 0.00998264 || Loss: 1.2335\n",
      "Epoch: [ 0/160] Iter [ 470/1487] || Time: 446.3987 sec || lr: 0.00998226 || Loss: 2.0606\n",
      "Epoch: [ 0/160] Iter [ 480/1487] || Time: 455.6156 sec || lr: 0.00998188 || Loss: 2.3229\n",
      "Epoch: [ 0/160] Iter [ 490/1487] || Time: 464.4721 sec || lr: 0.00998150 || Loss: 1.4055\n",
      "Epoch: [ 0/160] Iter [ 500/1487] || Time: 473.4311 sec || lr: 0.00998112 || Loss: 1.1862\n",
      "Epoch: [ 0/160] Iter [ 510/1487] || Time: 482.6928 sec || lr: 0.00998074 || Loss: 1.6601\n",
      "Epoch: [ 0/160] Iter [ 520/1487] || Time: 491.9745 sec || lr: 0.00998037 || Loss: 1.3415\n",
      "Epoch: [ 0/160] Iter [ 530/1487] || Time: 501.2782 sec || lr: 0.00997999 || Loss: 1.8277\n",
      "Epoch: [ 0/160] Iter [ 540/1487] || Time: 510.7985 sec || lr: 0.00997961 || Loss: 1.9350\n",
      "Epoch: [ 0/160] Iter [ 550/1487] || Time: 519.7521 sec || lr: 0.00997923 || Loss: 1.6860\n",
      "Epoch: [ 0/160] Iter [ 560/1487] || Time: 529.2267 sec || lr: 0.00997885 || Loss: 2.0521\n",
      "Epoch: [ 0/160] Iter [ 570/1487] || Time: 538.6524 sec || lr: 0.00997847 || Loss: 1.9551\n",
      "Epoch: [ 0/160] Iter [ 580/1487] || Time: 549.1233 sec || lr: 0.00997809 || Loss: 2.1943\n",
      "Epoch: [ 0/160] Iter [ 590/1487] || Time: 559.9923 sec || lr: 0.00997772 || Loss: 3.3731\n",
      "Epoch: [ 0/160] Iter [ 600/1487] || Time: 570.1808 sec || lr: 0.00997734 || Loss: 1.3734\n",
      "Epoch: [ 0/160] Iter [ 610/1487] || Time: 579.4218 sec || lr: 0.00997696 || Loss: 0.9840\n",
      "Epoch: [ 0/160] Iter [ 620/1487] || Time: 588.6798 sec || lr: 0.00997658 || Loss: 1.9220\n",
      "Epoch: [ 0/160] Iter [ 630/1487] || Time: 597.8837 sec || lr: 0.00997620 || Loss: 1.0616\n",
      "Epoch: [ 0/160] Iter [ 640/1487] || Time: 608.0863 sec || lr: 0.00997582 || Loss: 1.3880\n",
      "Epoch: [ 0/160] Iter [ 650/1487] || Time: 617.5386 sec || lr: 0.00997545 || Loss: 1.8426\n",
      "Epoch: [ 0/160] Iter [ 660/1487] || Time: 627.0188 sec || lr: 0.00997507 || Loss: 1.9168\n",
      "Epoch: [ 0/160] Iter [ 670/1487] || Time: 636.5899 sec || lr: 0.00997469 || Loss: 1.6123\n",
      "Epoch: [ 0/160] Iter [ 680/1487] || Time: 646.8123 sec || lr: 0.00997431 || Loss: 1.6827\n",
      "Epoch: [ 0/160] Iter [ 690/1487] || Time: 656.5754 sec || lr: 0.00997393 || Loss: 1.9299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/160 [11:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 199\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mStarting Epoch: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, Total Epochs: \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (args[\u001b[39m'\u001b[39m\u001b[39mstart_epoch\u001b[39m\u001b[39m'\u001b[39m], args[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m]))\n\u001b[1;32m--> 199\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "Cell \u001b[1;32mIn[7], line 114\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m targets \u001b[39m=\u001b[39m targets\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    113\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(images)\n\u001b[1;32m--> 114\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcriterion(outputs, targets)\n\u001b[0;32m    116\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m    117\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\VSCode\\Projects\\Visual Learning Project FastSCNN\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\VSCode\\Projects\\Visual Learning Project FastSCNN\\utils\\loss.py:114\u001b[0m, in \u001b[0;36mMixSoftmaxCrossEntropyOHEMLoss.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_aux_forward(\u001b[39m*\u001b[39minputs)\n\u001b[0;32m    113\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(MixSoftmaxCrossEntropyOHEMLoss, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49minputs)\n",
      "File \u001b[1;32md:\\VSCode\\Projects\\Visual Learning Project FastSCNN\\utils\\loss.py:76\u001b[0m, in \u001b[0;36mSoftmaxCrossEntropyOHEMLoss.forward\u001b[1;34m(self, predict, target, weight)\u001b[0m\n\u001b[0;32m     74\u001b[0m threshold \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthresh\n\u001b[0;32m     75\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_kept \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 76\u001b[0m     index \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39;49margsort()\n\u001b[0;32m     77\u001b[0m     threshold_index \u001b[39m=\u001b[39m index[\u001b[39mmin\u001b[39m(\u001b[39mlen\u001b[39m(index), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_kept) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m]\n\u001b[0;32m     78\u001b[0m     \u001b[39mif\u001b[39;00m pred[threshold_index] \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mthresh:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torchvision import transforms\n",
    "# from data_loader import get_segmentation_dataset\n",
    "# from models.fast_scnn import get_fast_scnn\n",
    "from utils.loss import MixSoftmaxCrossEntropyLoss, MixSoftmaxCrossEntropyOHEMLoss\n",
    "from utils.lr_scheduler import LRScheduler\n",
    "from utils.metric import SegmentationMetric\n",
    "\n",
    "\n",
    "def parse_args(model, dataset, base_size, crop_size, train_split):\n",
    "    \"\"\"Training Options for Segmentation Experiments\"\"\"\n",
    "    args = {\n",
    "        'model': model,\n",
    "        'dataset': dataset,\n",
    "        'base_size': base_size,\n",
    "        'crop_size': crop_size,\n",
    "        'train_split': train_split,\n",
    "        'aux': False,\n",
    "        'aux_weight': 0.4,\n",
    "        'epochs': 160,\n",
    "        'start_epoch': 0,\n",
    "        'batch_size': 2,\n",
    "        'lr': 1e-2,\n",
    "        'momentum': 0.9,\n",
    "        'weight_decay': 1e-4,\n",
    "        'resume': None,\n",
    "        'save_folder': './weights',\n",
    "        'eval': False,\n",
    "        'no_val': True\n",
    "    }\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    cudnn.benchmark = True\n",
    "    args['device'] = device\n",
    "    print(args)\n",
    "    return args\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        # image transform\n",
    "        input_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n",
    "        ])\n",
    "        # dataset and dataloader\n",
    "        data_kwargs = {'transform': input_transform, 'base_size': args['base_size'], 'crop_size': args['crop_size']}\n",
    "        train_dataset = get_segmentation_dataset(args['dataset'], split=args['train_split'], mode='train', **data_kwargs)\n",
    "        val_dataset = get_segmentation_dataset(args['dataset'], split='val', mode='val', **data_kwargs)\n",
    "        self.train_loader = data.DataLoader(dataset=train_dataset,\n",
    "                                            batch_size=args['batch_size'],\n",
    "                                            shuffle=True,\n",
    "                                            drop_last=True)\n",
    "        self.val_loader = data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False)\n",
    "\n",
    "        # create network\n",
    "        self.model = get_fast_scnn(dataset=args['dataset'], aux=args['aux'])\n",
    "        if torch.cuda.device_count() > 1:\n",
    "            self.model = torch.nn.DataParallel(self.model, device_ids=[0, 1, 2])\n",
    "        self.model.to(args['device'])\n",
    "\n",
    "        # resume checkpoint if needed\n",
    "        if args['resume']:\n",
    "            if os.path.isfile(args['resume']):\n",
    "                name, ext = os.path.splitext(args['resume'])\n",
    "                assert ext == '.pkl' or '.pth', 'Sorry only .pth and .pkl files supported.'\n",
    "                print('Resuming training, loading {}...'.format(args['resume']))\n",
    "                self.model.load_state_dict(torch.load(args['resume'], map_location=lambda storage, loc: storage))\n",
    "\n",
    "        # create criterion\n",
    "        self.criterion = MixSoftmaxCrossEntropyOHEMLoss(aux=args['aux'], aux_weight=args['aux_weight'],\n",
    "                                                        ignore_index=-1).to(args['device'])\n",
    "\n",
    "        # optimizer\n",
    "        self.optimizer = torch.optim.SGD(self.model.parameters(),\n",
    "                                         lr=args['lr'],\n",
    "                                         momentum=args['momentum'],\n",
    "                                         weight_decay=args['weight_decay'])\n",
    "\n",
    "        # lr scheduling\n",
    "        self.lr_scheduler = LRScheduler(mode='poly', base_lr=args['lr'], nepochs=args['epochs'],\n",
    "                                        iters_per_epoch=len(self.train_loader), power=0.9)\n",
    "\n",
    "        # evaluation metrics\n",
    "        self.metric = SegmentationMetric(train_dataset.num_class)\n",
    "\n",
    "        self.best_pred = 0.0\n",
    "\n",
    "    def train(self):\n",
    "        cur_iters = 0\n",
    "        start_time = time.time()\n",
    "        for epoch in tqdm(range(self.args['start_epoch'], self.args['epochs'])):\n",
    "            self.model.train()\n",
    "\n",
    "            for i, (images, targets) in enumerate(self.train_loader):\n",
    "                cur_lr = self.lr_scheduler(cur_iters)\n",
    "                for param_group in self.optimizer.param_groups:\n",
    "                    param_group['lr'] = cur_lr\n",
    "\n",
    "                images = images.to(self.args['device'])\n",
    "                targets = targets.to(self.args['device'])\n",
    "\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                cur_iters += 1\n",
    "                if cur_iters % 10 == 0:\n",
    "                    print('Epoch: [%2d/%2d] Iter [%4d/%4d] || Time: %4.4f sec || lr: %.8f || Loss: %.4f' % (\n",
    "                        epoch, self.args['epochs'], i + 1, len(self.train_loader),\n",
    "                        time.time() - start_time, cur_lr, loss.item()))\n",
    "\n",
    "            if self.args['no_val']:\n",
    "                # save every epoch\n",
    "                save_checkpoint(self.model, self.args, is_best=False)\n",
    "            else:\n",
    "                self.validation(epoch)\n",
    "\n",
    "        save_checkpoint(self.model, self.args, is_best=False)\n",
    "\n",
    "    def validation(self, epoch):\n",
    "        is_best = False\n",
    "        self.metric.reset()\n",
    "        self.model.eval()\n",
    "        for i, (image, target) in enumerate(self.val_loader):\n",
    "            image = image.to(self.args['device'])\n",
    "\n",
    "            outputs = self.model(image)\n",
    "            pred = torch.argmax(outputs[0], 1)\n",
    "            pred = pred.cpu().data.numpy()\n",
    "            self.metric.update(pred, target.numpy())\n",
    "            pixAcc, mIoU = self.metric.get()\n",
    "            print('Epoch %d, Sample %d, validation pixAcc: %.3f%%, mIoU: %.3f%%' % (\n",
    "                epoch, i + 1, pixAcc * 100, mIoU * 100))\n",
    "\n",
    "        new_pred = (pixAcc + mIoU) / 2\n",
    "        if new_pred > self.best_pred:\n",
    "            is_best = True\n",
    "            self.best_pred = new_pred\n",
    "        save_checkpoint(self.model, self.args, is_best)\n",
    "\n",
    "\n",
    "def save_checkpoint(model, args, is_best=False):\n",
    "    \"\"\"Save Checkpoint\"\"\"\n",
    "    directory = os.path.expanduser(args['save_folder'])\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    filename = '{}_{}.pth'.format(args['model'], args['dataset'])\n",
    "    save_path = os.path.join(directory, filename)\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    if is_best:\n",
    "        best_filename = '{}_{}_best_model.pth'.format(args['model'], args['dataset'])\n",
    "        best_filename = os.path.join(directory, best_filename)\n",
    "        shutil.copyfile(filename, best_filename)\n",
    "\n",
    "\n",
    "# Provide the arguments when calling parse_args function\n",
    "args = {\n",
    "    'model': 'fast_scnn',\n",
    "    'dataset': 'citys',\n",
    "    'base_size': 1024,\n",
    "    'crop_size': 768,\n",
    "    'train_split': 'train',\n",
    "    'aux': False,\n",
    "    'aux_weight': 0.4,\n",
    "    'epochs': 160,\n",
    "    'start_epoch': 0,\n",
    "    'batch_size': 2,\n",
    "    'lr': 1e-2,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 1e-4,\n",
    "    'resume': None,\n",
    "    'save_folder': './weights',\n",
    "    'eval': False,\n",
    "    'no_val': True,\n",
    "    'device': 'cuda'  # or 'cpu' if you want to use CPU\n",
    "\n",
    "}\n",
    "\n",
    "trainer = Trainer(args)\n",
    "if args['eval']:\n",
    "    print('Evaluation model: ', args['resume'])\n",
    "    trainer.validation(args['start_epoch'])\n",
    "else:\n",
    "    print('Starting Epoch: %d, Total Epochs: %d' % (args['start_epoch'], args['epochs']))\n",
    "    trainer.train()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATING THE MODEL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 images in the folder C:\\Fast-SCNN-pytorch\\datasets\\citys\\leftImg8bit/val\n",
      "Finished loading model!\n",
      "Testing model: fast_scnn\n",
      "Sample 1, validation pixAcc: 91.577%, mIoU: 32.990%\n",
      "Sample 2, validation pixAcc: 93.812%, mIoU: 37.468%\n",
      "Sample 3, validation pixAcc: 94.124%, mIoU: 36.406%\n",
      "Sample 4, validation pixAcc: 94.391%, mIoU: 40.400%\n",
      "Sample 5, validation pixAcc: 94.159%, mIoU: 39.653%\n",
      "Sample 6, validation pixAcc: 94.006%, mIoU: 39.582%\n",
      "Sample 7, validation pixAcc: 93.871%, mIoU: 40.923%\n",
      "Sample 8, validation pixAcc: 93.930%, mIoU: 41.746%\n",
      "Sample 9, validation pixAcc: 94.038%, mIoU: 43.351%\n",
      "Sample 10, validation pixAcc: 93.596%, mIoU: 45.530%\n",
      "Sample 11, validation pixAcc: 93.642%, mIoU: 46.469%\n",
      "Sample 12, validation pixAcc: 93.047%, mIoU: 48.380%\n",
      "Sample 13, validation pixAcc: 93.046%, mIoU: 48.906%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m evaluator \u001b[39m=\u001b[39m Evaluator()\n\u001b[0;32m     56\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTesting model: fast_scnn\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m evaluator\u001b[39m.\u001b[39;49meval()\n",
      "Cell \u001b[1;32mIn[19], line 46\u001b[0m, in \u001b[0;36mEvaluator.eval\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     43\u001b[0m pred \u001b[39m=\u001b[39m pred\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m     44\u001b[0m label \u001b[39m=\u001b[39m label\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m---> 46\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetric\u001b[39m.\u001b[39;49mupdate(pred, label)\n\u001b[0;32m     47\u001b[0m pixAcc, mIoU \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric\u001b[39m.\u001b[39mget()\n\u001b[0;32m     48\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mSample \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, validation pixAcc: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m, mIoU: \u001b[39m\u001b[39m%.3f\u001b[39;00m\u001b[39m%%\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, pixAcc \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m, mIoU \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m))\n",
      "File \u001b[1;32mc:\\Fast-SCNN-pytorch\\utils\\metric.py:33\u001b[0m, in \u001b[0;36mSegmentationMetric.update\u001b[1;34m(self, preds, labels)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Updates the internal evaluation result.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39m    Predicted values.\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(preds, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m---> 33\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_worker(preds, labels)\n\u001b[0;32m     34\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(preds, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m     35\u001b[0m     threads \u001b[39m=\u001b[39m [threading\u001b[39m.\u001b[39mThread(target\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluate_worker, args\u001b[39m=\u001b[39m(pred, label), )\n\u001b[0;32m     36\u001b[0m                \u001b[39mfor\u001b[39;00m (pred, label) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(preds, labels)]\n",
      "File \u001b[1;32mc:\\Fast-SCNN-pytorch\\utils\\metric.py:58\u001b[0m, in \u001b[0;36mSegmentationMetric.evaluate_worker\u001b[1;34m(self, pred, label)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_worker\u001b[39m(\u001b[39mself\u001b[39m, pred, label):\n\u001b[0;32m     57\u001b[0m     correct, labeled \u001b[39m=\u001b[39m batch_pix_accuracy(pred, label)\n\u001b[1;32m---> 58\u001b[0m     inter, union \u001b[39m=\u001b[39m batch_intersection_union(pred, label, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnclass)\n\u001b[0;32m     59\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlock:\n\u001b[0;32m     60\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtotal_correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m correct\n",
      "File \u001b[1;32mc:\\Fast-SCNN-pytorch\\utils\\metric.py:93\u001b[0m, in \u001b[0;36mbatch_intersection_union\u001b[1;34m(predict, target, nclass)\u001b[0m\n\u001b[0;32m     91\u001b[0m maxi \u001b[39m=\u001b[39m nclass\n\u001b[0;32m     92\u001b[0m nbins \u001b[39m=\u001b[39m nclass\n\u001b[1;32m---> 93\u001b[0m predict \u001b[39m=\u001b[39m predict\u001b[39m.\u001b[39;49mastype(\u001b[39m'\u001b[39;49m\u001b[39mint64\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     94\u001b[0m target \u001b[39m=\u001b[39m target\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mint64\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     96\u001b[0m predict \u001b[39m=\u001b[39m predict \u001b[39m*\u001b[39m (target \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mastype(predict\u001b[39m.\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "from torchvision import transforms\n",
    "# from data_loader import get_segmentation_dataset\n",
    "# from models.fast_scnn import get_fast_scnn\n",
    "from utils.metric import SegmentationMetric\n",
    "from utils.visualize import get_color_pallete\n",
    "\n",
    "\n",
    "class Evaluator(object):\n",
    "    def __init__(self):\n",
    "        # output folder\n",
    "        self.outdir = 'test_result'\n",
    "        if not os.path.exists(self.outdir):\n",
    "            os.makedirs(self.outdir)\n",
    "        # image transform\n",
    "        input_transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([.485, .456, .406], [.229, .224, .225]),\n",
    "        ])\n",
    "        # dataset and dataloader\n",
    "        val_dataset = get_segmentation_dataset('citys', split='val', mode='testval',\n",
    "                                               transform=input_transform)\n",
    "        self.val_loader = data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=1,\n",
    "                                          shuffle=False)\n",
    "        # create network\n",
    "        self.model = get_fast_scnn('citys', aux=False, pretrained=True, root='./weights').to('cuda')\n",
    "        print('Finished loading model!')\n",
    "\n",
    "        self.metric = SegmentationMetric(val_dataset.num_class)\n",
    "\n",
    "    def eval(self):\n",
    "        self.model.eval()\n",
    "        for i, (image, label) in enumerate(self.val_loader):\n",
    "            image = image.to('cuda')\n",
    "\n",
    "            outputs = self.model(image)\n",
    "\n",
    "            pred = torch.argmax(outputs[0], 1)\n",
    "            pred = pred.cpu().data.numpy()\n",
    "            label = label.numpy()\n",
    "\n",
    "            self.metric.update(pred, label)\n",
    "            pixAcc, mIoU = self.metric.get()\n",
    "            print('Sample %d, validation pixAcc: %.3f%%, mIoU: %.3f%%' % (i + 1, pixAcc * 100, mIoU * 100))\n",
    "\n",
    "            predict = pred.squeeze(0)\n",
    "            mask = get_color_pallete(predict, 'citys')\n",
    "            mask.save(os.path.join(self.outdir, 'seg_{}.png'.format(i)))\n",
    "\n",
    "\n",
    "evaluator = Evaluator()\n",
    "print('Testing model: fast_scnn')\n",
    "evaluator.eval()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEMO OUTPUT OF THE MODEL ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from utils.visualize import get_color_pallete\n",
    "\n",
    "def demo(model='fast_scnn', dataset='citys', weights_folder='./weights',\n",
    "         input_pic='./datasets/citys/leftImg8bit/test/berlin/berlin_000000_000019_leftImg8bit.png',\n",
    "         outdir='./test_result', cpu=False):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not cpu else \"cpu\")\n",
    "\n",
    "    # Output folder\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "\n",
    "    # Image transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    image = Image.open(input_pic).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    model = get_fast_scnn(dataset, pretrained=True, root=weights_folder, map_cpu=cpu).to(device)\n",
    "    print('Finished loading model!')\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image)\n",
    "\n",
    "    pred = torch.argmax(outputs[0], 1).squeeze(0).cpu().data.numpy()\n",
    "    mask = get_color_pallete(pred, dataset)\n",
    "    outname = os.path.splitext(os.path.split(input_pic)[-1])[0] + '.png'\n",
    "    mask.save(os.path.join(outdir, outname))\n",
    "\n",
    "# Call the demo function with the desired arguments\n",
    "demo(model='fast_scnn', dataset='citys', weights_folder='./weights',\n",
    "     input_pic='./datasets/citys/leftImg8bit/test/berlin/berlin_000000_000019_leftImg8bit.png',\n",
    "     outdir='./test_result', cpu=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
